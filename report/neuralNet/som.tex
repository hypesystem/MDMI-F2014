\section{Self Organizing Maps} % (fold)
\label{sec:self_organizing_maps}
The task of finding transitions between songs such that each song is similar to the last, is essentially a shortest path problem, which can be solved by constructing a graph. Each song could be a vertex in this graph, and the euclidean distance between songs could be used for edge weights. However, there is no obvious way to determine which songs should be connected. A single, connected graph must be constructed, so all vertices must be connected to all other vertices. Traditional shortest path algorithms would need considerable time to find a solution in such a graph, given the amount of data.
\\\\
Instead of the infeasible approach described above, we decided to use a self-organizing map (SOM) to represent the data. The SOM is a neural network giving a representation of the input space using fewer dimensions. Neurons are organized in a grid, typically such that each neuron has four neighbours. Each neuron has a so called prototype vector, with the same dimensionality as the input data. The network is trained by repeatedly finding the most similar neuron to training data points, and then updating that neuron's prototype vector to make it more similar to the data vector. Neighbours to the best match neuron's prototype vectors are also updated, using a neighbourhood function, making neighbouring neurons more similar to the best-match neuron.
\\\\
The SOM is useful for visualising topological features of high-dimensional data, because the features are mapped to e.g a two dimensional grid. A common way of visualising the SOM is to construct a u-matrix. In this approach, a height is associated with each neuron. The height is the average of the euclidean distances to each of its closest neighbours. The result is a 3-dimensional landscape which can be coloured with respect to the heights. This visualisation clearly shows topological features of the map, in that areas in the network where neurons are similar will be shown as valleys in the u-matrix, where all the heights are small. Large differences between neurons will result in large heights, that show up as mountains in the u-matrix (see Figure \ref{fig:map}).
\\\\
The important quality of the self organizing map for this purpose, is that the map preserves the topological features of the input space. This means that we can search for paths in the SOM or in the u-matrix, instead of searching directly in the input data. Moreover, the SOM provides an obvious way of constructing a graph, since the neurons themselves can be thought of as vertices in a graph, and neighbourhood relationships between neurons can be thought of as edges.

\subsection{Parametrisation} % (fold)
\label{sub:parametrisation}

ESOM allows the SOM training to be parametrised in a number of different ways. One way we found to be significant for our purposes was the size of the map. We found that a small map (fewer neurons) gave less meaningful results than a larger map (more neurons) when using it to find shortest paths between songs. This might be related to the difference in topological fidelity to the input data between smaller and larger maps.
\\\\
There are other parametrisation options for SOM training, such as different learning rate cooling functions, neighbourhood kernel functions and grid type, such as toroid vs. borders. We have not experimented with these parameters, but they might also have an effect on the topological features of the map. It could be an interesting continuation of the project to see whether tweaking these could lead to better playlists.

% subsection parametrisation (end)

% section self_organizing_maps (end)